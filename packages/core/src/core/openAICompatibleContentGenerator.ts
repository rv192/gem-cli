import {
  CountTokensResponse,
  GenerateContentResponse,
  GenerateContentParameters,
  CountTokensParameters,
  EmbedContentResponse,
  EmbedContentParameters,
  Content,
  Part,
  ContentListUnion,
  PartUnion,
} from '@google/genai';
import OpenAI from 'openai';
import { ContentGenerator, AuthType } from './contentGenerator.js';
import { jsonrepair } from 'jsonrepair';
import { reportError } from '../utils/errorReporting.js';

/**
 * Helper function to convert ContentListUnion to Content[]
 */
function toContents(contents: ContentListUnion): Content[] {
  if (Array.isArray(contents)) {
    // it's a Content[] or a PartUnion[]
    return contents.map(toContent);
  }
  // it's a Content or a PartUnion
  return [toContent(contents)];
}

function toContent(content: Content | PartUnion): Content {
  if (Array.isArray(content)) {
    // This shouldn't happen in our context, but handle it
    throw new Error('Array content not supported in this context');
  }
  if (typeof content === 'string') {
    // it's a string
    return {
      role: 'user',
      parts: [{ text: content }],
    };
  }
  if (typeof content === 'object' && content !== null && 'parts' in content) {
    // it's a Content
    return content;
  }
  // it's a Part
  return {
    role: 'user',
    parts: [content as Part],
  };
}

export class OpenAICompatibleContentGenerator implements ContentGenerator {
  private openai: OpenAI;
  private fallbackModels: string[];
  private defaultModel: string; // å­˜å‚¨ç¡®å®šçš„é»˜è®¤æ¨¡å‹

  constructor(authType?: AuthType) {
    let apiKey: string;
    let baseUrl: string;
    let defaultModel: string;
    let fallbackModels: string[];

    // æ ¹æ®è®¤è¯ç±»å‹å†³å®šé…ç½®
    if (authType === AuthType.USE_SILICONFLOW) {
      // å¼ºåˆ¶ä½¿ç”¨ SiliconFlow é…ç½®
      apiKey = process.env.SILICONFLOW_API_KEY || 'sk-ybhnlsuxeobtrbijnowwrvloegnguaihmjvervuhqqzrhzqm';
      baseUrl = process.env.SILICONFLOW_BASE_URL || 'https://api.siliconflow.cn';
      defaultModel = process.env.SILICONFLOW_DEFAULT_MODEL || 'THUDM/GLM-4-9B-0414';
      fallbackModels = []; // SiliconFlow ä¸ä½¿ç”¨å›é€€æ¨¡å‹
      console.log('ğŸ” OpenAICompatibleContentGenerator - ä½¿ç”¨ SiliconFlow API é…ç½®');
      console.log('ğŸ” OpenAICompatibleContentGenerator - SiliconFlow defaultModel:', defaultModel);
      console.log('ğŸ” OpenAICompatibleContentGenerator - SiliconFlow fallbackModels:', fallbackModels);
    } else if (authType === AuthType.USE_OPENAI_COMPATIBLE) {
      // å¼ºåˆ¶ä½¿ç”¨ OpenAI å…¼å®¹é…ç½®
      apiKey = process.env.OPENAI_API_KEY || '';
      baseUrl = process.env.OPENAI_BASE_URL || 'https://api.openai.com';
      defaultModel = process.env.DEFAULT_MODEL || 'gpt-4o';
      const fallbackModelsEnv = process.env.FALLBACK_MODELS;
      fallbackModels = fallbackModelsEnv ? fallbackModelsEnv.split(',').map(m => m.trim()) : [
        'gpt-4-turbo',
        'gpt-3.5-turbo'
      ];
      console.log('ä½¿ç”¨ OpenAI å…¼å®¹ API é…ç½®ã€‚');
    } else {
      // å¦‚æœæ²¡æœ‰æŒ‡å®šè®¤è¯ç±»å‹ï¼ŒæŠ›å‡ºé”™è¯¯
      throw new Error(`æœªçŸ¥çš„è®¤è¯ç±»å‹: ${authType}ã€‚è¯·æŒ‡å®šæœ‰æ•ˆçš„è®¤è¯ç±»å‹ã€‚`);
    }

    // ç¡®ä¿ baseURL ä»¥ /v1 ç»“å°¾ï¼Œä»¥å…¼å®¹ OpenAI å®¢æˆ·ç«¯
    const normalizedBaseUrl = baseUrl.endsWith('/v1') ? baseUrl : `${baseUrl}/v1`;
    this.openai = new OpenAI({
      apiKey,
      baseURL: normalizedBaseUrl,
    });

    this.fallbackModels = fallbackModels;
    this.defaultModel = defaultModel; // å­˜å‚¨ç¡®å®šçš„é»˜è®¤æ¨¡å‹

    console.log('ğŸ” OpenAICompatibleContentGenerator - Final this.defaultModel:', this.defaultModel);
    console.log('ğŸ” OpenAICompatibleContentGenerator - Final this.fallbackModels:', this.fallbackModels);
  }

  private async tryWithFallbackModels<T>(
    requestedModel: string | undefined, // ç”¨æˆ·è¯·æ±‚çš„æ¨¡å‹ï¼Œå¯èƒ½ä¸º undefined
    operation: (model: string) => Promise<T>
  ): Promise<T> {
    // ç¡®å®šæœ¬æ¬¡æ“ä½œè¦ä½¿ç”¨çš„åˆå§‹æ¨¡å‹
    const initialModel = requestedModel || this.defaultModel;
    // å°è¯•çš„æ¨¡å‹åˆ—è¡¨ï¼šé¦–å…ˆæ˜¯åˆå§‹æ¨¡å‹ï¼Œç„¶åæ˜¯æ„é€ å‡½æ•°ä¸­ç¡®å®šçš„å›é€€æ¨¡å‹åˆ—è¡¨
    const modelsToTry = [initialModel, ...this.fallbackModels];
    let lastError: Error | null = null;

    console.log('ğŸ” tryWithFallbackModels - requestedModel:', requestedModel);
    console.log('ğŸ” tryWithFallbackModels - this.defaultModel:', this.defaultModel);
    console.log('ğŸ” tryWithFallbackModels - initialModel:', initialModel);
    console.log('ğŸ” tryWithFallbackModels - this.fallbackModels:', this.fallbackModels);
    console.log('ğŸ” tryWithFallbackModels - modelsToTry:', modelsToTry);

    for (const model of modelsToTry) {
      try {
        console.log(`å°è¯•æ¨¡å‹: ${model}`);
        return await operation(model);
      } catch (error) {
        lastError = error as Error;
        const errorMessage = error instanceof Error ? error.message : String(error);
        const errorString = JSON.stringify(error);

        console.log(`æ¨¡å‹ ${model} é”™è¯¯: ${errorMessage}`);
        console.log(`é”™è¯¯è¯¦æƒ…: ${errorString}`);

        // æ£€æŸ¥æ˜¯å¦æ˜¯æ¨¡å‹è€—å°½æˆ–æµå¼ä¼ è¾“é”™è¯¯ï¼Œå¹¶ä¸”åªæœ‰åœ¨é…ç½®äº†å›é€€æ¨¡å‹æ—¶æ‰å°è¯•å›é€€
        if (this.fallbackModels.length > 0 && (
            errorMessage.includes('Streaming failed') ||
            errorMessage.includes('rate limit') ||
            errorMessage.includes('quota') ||
            errorMessage.includes('exhausted') ||
            errorMessage.includes('Internal server error') ||
            errorMessage.includes('API Error') ||
            errorString.includes('Streaming failed'))) {
          console.log(`æ¨¡å‹ ${model} å¤±è´¥: ${errorMessage}, å°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹...`);
          continue;
        }

        // å¯¹äºå…¶ä»–é”™è¯¯ï¼Œæˆ–è€…å¦‚æœæ²¡æœ‰é…ç½®å›é€€æ¨¡å‹ï¼Œåˆ™ä¸å°è¯•å›é€€æ¨¡å‹
        console.log(`æ¨¡å‹ ${model} å¤±è´¥ï¼ŒåŸå› ä¸å¯é‡è¯•æˆ–æœªé…ç½®å›é€€: ${errorMessage}`);
        throw error;
      }
    }

    // å¦‚æœæ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥äº†ï¼ŒæŠ›å‡ºæœ€åä¸€ä¸ªé”™è¯¯
    throw lastError || new Error('æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥äº†');
  }

  private convertToOpenAIMessages(
    contents: Content[],
  ): OpenAI.Chat.Completions.ChatCompletionMessageParam[] {
    const messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = [];
    for (const content of contents) {
      const role =
        content.role === 'model'
          ? 'assistant'
          : (content.role as 'system' | 'user');
      const parts = content.parts || [];
      const textParts = parts.filter(
        (part: Part): part is { text: string } =>
          typeof part === 'object' && part !== null && 'text' in part,
      );
      if (textParts.length > 0) {
        const combinedText = textParts
          .map((part: { text: string }) => {
            // ç¡®ä¿æ–‡æœ¬æ­£ç¡®ç¼–ç ï¼Œé¿å… ByteString è½¬æ¢é”™è¯¯
            try {
              // ä½¿ç”¨ JSON.stringify å’Œ JSON.parse æ¥ç¡®ä¿ Unicode å­—ç¬¦æ­£ç¡®å¤„ç†
              return JSON.parse(JSON.stringify(part.text));
            } catch {
              // å¦‚æœ JSON å¤„ç†å¤±è´¥ï¼Œç›´æ¥è¿”å›åŸæ–‡æœ¬
              return part.text;
            }
          })
          .join('\n');
        messages.push({
          role,
          content: combinedText,
        });
      }

      const functionResponseParts = parts.filter(
        (
          part: Part,
        ): part is {
          functionResponse: {
            id: string;
            name: string;
            response: { output?: string; error?: string };
          };
        } =>
          typeof part === 'object' &&
          part !== null &&
          'functionResponse' in part &&
          part.functionResponse !== undefined &&
          typeof part.functionResponse.id === 'string' &&
          typeof part.functionResponse.name === 'string' &&
          part.functionResponse.response !== undefined &&
          (typeof part.functionResponse.response.output === 'string' ||
            typeof part.functionResponse.response.error === 'string'),
      );

      if (functionResponseParts.length > 0) {
        const combinedText = functionResponseParts
          .map((part) =>
            part.functionResponse.response.error
              ? `Error: ${part.functionResponse.response.error}`
              : part.functionResponse.response.output,
          )
          .join('\n');
        const tool_call_id = functionResponseParts[0].functionResponse.id;
        messages.push({
          tool_call_id,
          role: 'tool',
          content: combinedText,
        });
      }
      const functionCallParts = parts.filter(
        (
          part: Part,
        ): part is {
          functionCall: { name: string; args: Record<string, unknown> };
        } =>
          typeof part === 'object' &&
          part !== null &&
          'functionCall' in part &&
          part.functionCall !== undefined &&
          typeof part.functionCall.name === 'string' &&
          part.functionCall.args !== undefined,
      );

      if (functionCallParts.length > 0) {
        if (role === 'user') {
          throw new Error('Function calls cannot come from user role');
        }
        messages.push({
          role: 'assistant', // Force assistant role for tool calls
          content: null,
          tool_calls: functionCallParts.map((part) => ({
            id: `call_${Math.random().toString(36).slice(2)}`,
            type: 'function',
            function: {
              name: part.functionCall.name,
              arguments: JSON.stringify(part.functionCall.args),
            },
          })),
        });
      }

      if (
        textParts.length === 0 &&
        functionCallParts.length === 0 &&
        functionResponseParts.length === 0
      ) {
        throw new Error(
          `Content parts not processed: ${JSON.stringify(content, null, 2)}`,
        );
      }
    }

    return messages;
  }

  private convertToGeminiResponse(
    response: OpenAI.Chat.Completions.ChatCompletion,
  ): GenerateContentResponse {
    const choice = response.choices[0];
    if (!choice || (!choice.message.content && !choice.message.tool_calls)) {
      throw new Error('No valid choices in OpenAI response');
    }

    const geminiResponse = new GenerateContentResponse();

    if (choice.message.content) {
      geminiResponse.candidates = [
        {
          content: {
            parts: [{ text: choice.message.content }],
            role: 'model',
          },
          index: 0,
          safetyRatings: [],
        },
      ];
    } else if (choice.message.tool_calls) {
      geminiResponse.candidates = [
        {
          content: {
            parts: choice.message.tool_calls.map((toolCall) => ({
              functionCall: {
                name: toolCall.function.name,
                args: JSON.parse(jsonrepair(toolCall.function.arguments)),
              },
            })),
            role: 'model',
          },
          index: 0,
          safetyRatings: [],
        },
      ];
    }

    geminiResponse.usageMetadata = {
      promptTokenCount: response.usage?.prompt_tokens || 0,
      candidatesTokenCount: response.usage?.completion_tokens || 0,
      totalTokenCount: response.usage?.total_tokens || 0,
    };

    return geminiResponse;
  }
  async generateContentStream(
    request: GenerateContentParameters,
  ): Promise<AsyncGenerator<GenerateContentResponse>> {
    const contentsArray = toContents(request.contents);
    const messages = this.convertToOpenAIMessages(contentsArray);
    const tools: OpenAI.Chat.Completions.ChatCompletionTool[] | undefined =
      request.config?.tools?.flatMap((tool) => {
        if ('functionDeclarations' in tool) {
          return (
            tool.functionDeclarations?.map((func) => {
              if (!func.name) {
                throw new Error('Function declaration must have a name');
              }
              return {
                type: 'function',
                function: {
                  name: func.name,
                  description: func.description || '',
                  parameters:
                    (func.parameters as Record<string, unknown>) || {},
                },
              };
            }) || []
          );
        }
        return [];
      });

    // ä½¿ç”¨å›é€€æœºåˆ¶è¿›è¡Œæµå¼ä¼ è¾“
    const stream = await this.tryWithFallbackModels(request.model, async (model) => {
      const params = {
        model,
        messages,
        stream: true,
        temperature: request.config?.temperature,
        max_tokens: request.config?.maxOutputTokens,
        top_p: request.config?.topP,
        tools,
      };

      return await this.openai.chat.completions.create({
        ...params,
        stream: true,
      });
    });

    const toolCallMap = new Map<
      number,
      {
        name: string;
        arguments: string;
      }
    >();
    const generator =
      async function* (): AsyncGenerator<GenerateContentResponse> {
        for await (const chunk of stream) {
          const choice = chunk.choices[0];
          if (choice?.delta?.content) {
            const geminiResponse = new GenerateContentResponse();
            geminiResponse.candidates = [
              {
                content: {
                  parts: [{ text: choice.delta.content }],
                  role: 'model',
                },
                index: 0,
                safetyRatings: [],
              },
            ];
            yield geminiResponse;
          }
          // å¤„ç†å·¥å…·è°ƒç”¨å¢é‡
          if (choice?.delta?.tool_calls) {
            for (const toolCall of choice.delta.tool_calls) {
              const idx = toolCall.index;
              const current = toolCallMap.get(idx) || {
                name: '',
                arguments: '',
              };

              // å¦‚æœæä¾›äº†åç§°ï¼Œåˆ™æ›´æ–°åç§°
              if (toolCall.function?.name) {
                current.name = toolCall.function.name;
              }

              // ç´¯ç§¯å‚æ•°
              if (toolCall.function?.arguments) {
                current.arguments += toolCall.function.arguments;
              }

              toolCallMap.set(idx, current);
            }
          }

          const tryRepair = (str: string) => {
            try {
              return JSON.parse(jsonrepair(str));
            } catch (error) {
              reportError(
                error,
                'ä¸ OpenAI å…¼å®¹ API é€šä¿¡æ—¶å‡ºé”™',
                { str },
                'OpenAICompatible.parseToolCallArguments',
              );
              throw error;
            }
          };
          // åœ¨å®Œæˆæ—¶åˆ·æ–°å·²å®Œæˆçš„å·¥å…·è°ƒç”¨
          if (choice.finish_reason === 'tool_calls' && toolCallMap.size > 0) {
            const geminiResponse = new GenerateContentResponse();
            geminiResponse.candidates = [
              {
                content: {
                  parts: Array.from(toolCallMap.entries()).map(
                    ([_index, toolCall]) => ({
                      functionCall: {
                        name: toolCall.name,
                        args: toolCall.arguments
                          ? tryRepair(toolCall.arguments)
                          : {},
                      },
                    }),
                  ),
                  role: 'model',
                },
                index: 0,
                safetyRatings: [],
              },
            ];
            yield geminiResponse;
            toolCallMap.clear(); // ä¸ºä¸‹ä¸€æ¬¡å·¥å…·è°ƒç”¨é‡ç½®
          }

          if (choice?.finish_reason) {
            const geminiResponse = new GenerateContentResponse();
            geminiResponse.candidates = [
              {
                content: {
                  parts: [],
                  role: 'model',
                },
                index: 0,
                safetyRatings: [],
              },
            ];
            yield geminiResponse;
            return;
          }
        }
      };

    return generator();
  }

  async generateContent(
    request: GenerateContentParameters,
  ): Promise<GenerateContentResponse> {
    const contentsArray = toContents(request.contents);
    const messages = this.convertToOpenAIMessages(contentsArray);

    const tools = undefined;

    // ä½¿ç”¨å›é€€æœºåˆ¶è¿›è¡Œéæµå¼è¯·æ±‚
    const completion = await this.tryWithFallbackModels(request.model, async (model) => {
      return await this.openai.chat.completions.create({
        model,
        messages,
        stream: false,
        temperature: request.config?.temperature,
        max_tokens: request.config?.maxOutputTokens,
        top_p: request.config?.topP,
        tools,
      });
    });

    return this.convertToGeminiResponse(completion);
  }

  async countTokens(
    request: CountTokensParameters,
  ): Promise<CountTokensResponse> {
    const contentsArray = toContents(request.contents);

    // æˆ‘ä»¬å°†æ ¹æ®æ–‡æœ¬é•¿åº¦è¿›è¡Œä¼°ç®—ï¼ˆç²—ç•¥è¿‘ä¼¼ï¼šæ¯ 4 ä¸ªå­—ç¬¦ä¸€ä¸ª tokenï¼‰
    const messages = this.convertToOpenAIMessages(contentsArray);
    const totalText = messages.map((m) => m.content).join(' ');
    const estimatedTokens = Math.ceil(totalText.length / 4);

    return {
      totalTokens: estimatedTokens,
    };
  }

  async embedContent(
    _request: EmbedContentParameters,
  ): Promise<EmbedContentResponse> {
    throw new Error('TODO: add support for embedding content');
  }
}